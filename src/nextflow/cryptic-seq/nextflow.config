manifest {
    name = 'cryptic-seq'
    author = 'Tome Biosciences'
    description = 'Cryptic-seq Pipeline'
    version = '1.0.0'
    nextflowVersion = '>=22.04.0' // DSL2 is the default starting here
}

plugins {  
    id 'nf-schema@2.0.0'
}

params {
    // if true, show the help message and exit
    help = false
    // whether to run the version of the pipeline that uses
    // the wrapper around the snakemake workflow - primarily
    // for regression testing
    snakemake = false
    // options for executing with metadata from Benchling
    ctb_id = null
    genomes_json = null
    default_genome = null
    // options for executing with a metasheet
    metasheet = null
    // options to identify local inputs given a metasheet
    fastq_dir = null
    fastq_name_pattern = null
    attachment_sites = "${baseDir}/attachment-sites.json"
    // optional semicolon-separated list of default attachment site(s)
    default_attachment_site = null
    references_dir = null
    references_json = null
    default_reference = null
    // reference to use for searching for exact site matches during annotation step
    annotation_reference = null
    // whether to verify that reference and FASTQ files exist
    verify_files = false
    // output prefix - defaults to metasheet.simpleName
    prefix = null
    // dir where outputs should be linked/uploaded - defaults to launchDir/runID,
    // where runID is either the ctb_id or the metasheet filename
    output_dir = null
    // global snakemake config yaml
    global_config = null
    // option to specify the number of reads per chunk, to parallelize trimming of FASTQ files - 
    // when set, FASTQ files will be split prior to the `fastq_to_ubam` step and merged to a
    // single uBAM prior to the alignment step
    fastq_chunk_size = null
    // parameters that can also be set in the global config when running
    // the snakemake workflow
    trim_Tn5 = false  // TODO: rename to `trim_Tn5_r1`
    trim_Tn5_max_mismatches = 1
    trim_att_max_mismatches = 4
    read_structure_r1 = '11M+T'
    read_structure_r2 = '+T'
    umi_from_read_name = false
    // other global parameters
    trim_Tn5_r2 = true
    trim_Tn5_r2_min_score = 12  // TODO: too low?
    trim_Tn5_r2_min_match_length = 18
    trim_Tn5_r2_min_keep_length = null
    // sequence of r2 adapter to trim
    trim_adapter_r2 = null
    trim_adapter_r2_min_score = null  // TODO: too low?
    trim_adapter_r2_min_match_length = null
    trim_adapter_r2_min_keep_length = 25

    // Docker images
    docker_image_version = "1.0"  // for now, use the same version for all images
    fastqc_image = "tomebio/fastqc:${docker_image_version}"
    multiqc_image = "tomebio/multiqc:${docker_image_version}"
    picard_image = "tomebio/picard:${docker_image_version}"
    fgbio_image = "tomebio/fgbio:${docker_image_version}"
    tools_image = "tomebio/tools:${docker_image_version}"

    // Default and max resources specified as 'cpus;memory;disk;time'
    // Processes only need to specify requirements (within ext) if they are above default levels
    default_resources = '1;4.GB;100.GB;1.h'
    max_resources = '16;64.GB;1.TB;4.h'

    // Scale resources after this attempt, to deal with failures that are
    // due to insufficient resources. Set to `null` to never scale.
    scale_after_attempt = 1
}

profiles {
    local {
        docker {
            enabled = true
        }

        params {
            verify_files = true
        }

        process {
            cache = 'lenient'
            executor = 'local'
            errorStrategy = 'retry'
            maxRetries = 2
            
            // TODO: this requires nextflow 24.04.0, which is not yet available in bioconda.
            // This will replace params.max_resources.
            // resourceLimits = [ cpus: 16, memory: 64.GB, disk: 1.TB, time: 4.h ]

            // resource allocations scaled by retry attempt
            cpus = {
                calc_resource(
                    'cpus',
                    string_or_null(task.ext.cpus),
                    "${params.default_resources}",
                    "${params.max_resources}",
                    task.attempt
                )
            }
            memory = {
                calc_resource(
                    'memory',
                    string_or_null(task.ext.memory),
                    "${params.default_resources}",
                    "${params.max_resources}",
                    task.attempt
                )
            }
            disk = {
                calc_resource(
                    'disk',
                    string_or_null(task.ext.disk),
                    "${params.default_resources}",
                    "${params.max_resources}",
                    task.attempt
                )
            }
            time = {
                calc_resource(
                    'time',
                    string_or_null(task.ext.time),
                    "${params.default_resources}",
                    "${params.max_resources}",
                    task.attempt
                )
            }

            // publishDir for per-sample processes
            publishDir = { get_publish_dir(params, task.process, launchDir, meta.id) }
            
            // publishDir for aggregate processes
            withLabel: global {
                publishDir = { get_publish_dir(params, task.process, launchDir) }
            }

            // container configuration for native workflow processes
            withLabel: fastqc {
                container = "${params.fastqc_image}"
            }
            withLabel: multiqc {
                container = "${params.multiqc_image}"
            }
            withLabel: picard {
                container = "${params.picard_image}"
            }
            withLabel: fgbio {
                container = "${params.fgbio_image}"
            }
            withLabel: tools {
                container = "${params.tools_image}"
            }

            // publishDir and container for wrapped snakemake pipeline    
            withName: snakemake_pipeline {
                publishDir = { "${params.output_dir ?: workflow.launchDir}/${prefix}" }
                container = 'tomebio/cryptic-seq:1.0'
            }
        }
    }

    // Use this profile when running on MacOS under Rosetta emulation.
    rosetta {
        docker {
            platform = 'linux/amd64'
        }
    }
    
    // use this profile when running on linux
    // alteratively, configure docker on linux so that it does not require running as root:
    // https://docs.docker.com/engine/install/linux-postinstall/
    linux {
        docker {
            runOptions = '--user root'
        }
    }

    snakemake {
        params {
            snakemake = true
            // reset these to null here - the defaults will come from the global config
            trim_Tn5 = null
            trim_Tn5_max_mismatches = null
            trim_att_max_mismatches = null
            read_structure_r1 = null
            read_structure_r2 = null
            umi_from_read_name = null
        }
    }
}

includeConfig 'tests/nextflow.config'

def get_publish_dir(params, processName, launchDir, sampleName = null) {
    def outputDir = params.output_dir
    if (outputDir == null) {
        def name = params.ctb_id ?: params.metasheet.replaceAll(/^(?:.*\/)?([^.]+).*/, '$1')
        outputDir = "${launchDir}/${name}"
    }
    if (sampleName != null) {
        outputDir = "${outputDir}/${sampleName}"
    }
    def sepIdx = processName.lastIndexOf(':')
    if (sepIdx >= 0) {
        processName = processName.substring(sepIdx + 1)
    }
    outputDir = "${outputDir}/${processName}"
    return outputDir
}

def string_or_null(value) {
    if (value == null) {
        return null
    } else {
        return "${value}"
    }
}

def calc_resource(
    String type, String base_value, String default_values, String max_values, Integer attempt
) {
    def default_array = default_values.split(';')
    def max_array = max_values.split(';')
    def value = null
    def max_value = null
    if (type == 'cpus') {
        value = (base_value ?: default_array[0]) as Integer
        max_value = max_array[0] as Integer
    } else if (type == 'memory') {
        value = (base_value ?: default_array[1]) as nextflow.util.MemoryUnit
        max_value = max_array[1] as nextflow.util.MemoryUnit
    } else if (type == 'disk') {
        value = (base_value ?: default_array[2]) as nextflow.util.MemoryUnit
        max_value = max_array[2] as nextflow.util.MemoryUnit
    } else if (type == 'time') {
        value = (base_value ?: default_array[3]) as nextflow.util.Duration
        max_value = max_array[3] as nextflow.util.Duration
    } else {
        exit 1, "Unknown resource type ${type} passed to calc_resource"
    }
    // Increase resource allocation after the first attempt, with the
    // intent to resolve failures due to insufficient resources.
    if (params.scale_after_attempt == null || attempt <= params.scale_after_attempt) {
        attempt = 1
    } else {
        attempt = attempt - params.scale_after_attempt + 1
    }
    return [value * attempt, max_value].min()
}